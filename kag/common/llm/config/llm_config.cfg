
#-----------------------------------------------------------------------------------#
# openai SDK maas.   client_type = maas                                             #
                                                                                    #
# TongYi                                                                            #
[llm]                                                                               #
client_type = maas                                                                  #
base_url = https://dashscope.aliyuncs.com/compatible-mode/v1/chat/completions       #
api_key = "put your tongyi api key here"                                            #
model = qwen-turbo                                                                  #                                                                              #
                                                                                    #
# Deepseek                                                                          #
[llm]                                                                               #
client_type = maas                                                                  #
base_url = https://api.deepseek.com/beta                                            #
api_key = "put your deepseek api key here"                                          #
model = deepseek-chat                                                               #
                                                                                    #
# OpenAI                                                                            #
[llm]                                                                               #
client_type = maas                                                                  #
base_url = https://api.openai.com/v1/chat/completions                               #
api_key = "put your openai api key here"                                            #
model = gpt-3.5-turbo                                                               #
                                                                                    #
#-----------------------------------------------------------------------------------#




#-----------------------------------------------------------------------------------#
# local llm service.    client_type = vllm                                          #
                                                                                    #
# vllm                                                                              #
[llm]                                                                               #
client_type = vllm                                                                  #
base_url = http://localhost:8000/v1/chat/completions                                #
model = qwen-7b-chat                                                                #
                                                                                    #
#-----------------------------------------------------------------------------------#




#-----------------------------------------------------------------------------------#
# maya llm service.    client_type = maya                                           #
                                                                                    #
[llm]                                                                               #
client_type = maya                                                                  #
scene_name = Qwen2_7B_Instruct_Knowledge                                            #
chain_name = v1                                                                     #
lora_name = humming-v25                                                             #
                                                                                    #
#-----------------------------------------------------------------------------------#




#-----------------------------------------------------------------------------------#
                                                                                    #
# ollama                                                                            #
[llm]
client_type = ollama
base_url = http://localhost:11434/api/generate
model = llama3.1                                                                #
                                                                                    #
#-----------------------------------------------------------------------------------#
